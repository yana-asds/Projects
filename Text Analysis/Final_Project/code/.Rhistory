ksTest(empirical)
## Load packages
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse",
"guardianapi", # for working with the Guardian's API
"quanteda", # for QTA
"quanteda.textstats", # more Quanteda!
"quanteda.textplots", # even more Quanteda!
"readtext", # for reading in text data
"stringi", # for working with character strings
"textstem" # an alternative method for lemmatizing
), pkgTest)
dat <- gu_content(query = "Ukraine", from_date = "2024-01-01") # making a tibble
7c26e710-f5ed-49fe-a725-2986c6a01ffa# We want to query the API on articles featuring Ukraine since Jan 1 2024
## Load packages
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse",
"guardianapi", # for working with the Guardian's API
"quanteda", # for QTA
"quanteda.textstats", # more Quanteda!
"quanteda.textplots", # even more Quanteda!
"readtext", # for reading in text data
"stringi", # for working with character strings
"textstem" # an alternative method for lemmatizing
), pkgTest)
7c26e710-f5ed-49fe-a725-2986c6a01ffa
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(error = TRUE)
## Load packages
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse",
"guardianapi", # for working with the Guardian's API
"quanteda", # for QTA
"quanteda.textstats", # more Quanteda!
"quanteda.textplots", # even more Quanteda!
"readtext", # for reading in text data
"stringi", # for working with character strings
"textstem" # an alternative method for lemmatizing
), pkgTest)
7c26e710-f5ed-49fe-a725-2986c6a01ffa
# 7c26e710-f5ed-49fe-a725-2986c6a01ffa
dat <- gu_content(query = "Ukraine", from_date = "2024-01-01")
## Load packages
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse",
"guardianapi", # for working with the Guardian's API
"quanteda", # for QTA
"quanteda.textstats", # more Quanteda!
"quanteda.textplots", # even more Quanteda!
"readtext", # for reading in text data
"stringi", # for working with character strings
"textstem" # an alternative method for lemmatizing
), pkgTest)
# 7c26e710-f5ed-49fe-a725-2986c6a01ffa
dat <- gu_content(query = "London", from_date = "2013-01-01")
View(stmdfm)
View(dfm)
View(dfm)
## Load packages
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse",
"guardianapi", # for working with the Guardian's API
"quanteda", # for QTA
"quanteda.textstats", # more Quanteda!
"quanteda.textplots", # even more Quanteda!
"readtext", # for reading in text data
"stringi", # for working with character strings
"textstem" # an alternative method for lemmatizing
), pkgTest)
## Load packages
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse",
"guardianapi", # for working with the Guardian's API
"quanteda", # for QTA
"quanteda.textstats", # more Quanteda!
"quanteda.textplots", # even more Quanteda!
"readtext", # for reading in text data
"stringi", # for working with character strings
"textstem" # an alternative method for lemmatizing
), pkgTest)
### A. Using the Guardian API with R
gu_api_key() # run this interactive function### A. Using the Guardian API with R
## Load packages
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# Acquire stmBrowser package from github
if(!require(devtools)) install.packages("devtools")
library(devtools)
install_github("mroberts/stmBrowser",dependencies=TRUE)
lapply(c("tidyverse",
"quanteda",
"quanteda.textstats",
"lubridate",
"stm",
"wordcloud",
"stmBrowser",
"LDAvis"),
pkgTest)
## 1. Read in and wrangle data
#     a) In the data folder you'll find a large data.frame object called
#        ukr_h1_2022. Read it in, and check the type of articles it contains.
dat <- readRDS("/Users/yana/Documents/01_TRINITY/03_Quantitative Text Analysis tutorial/tutorial05/data/ukr_h1_2022")
#     b) Pre-process the data.frame.
dat$body_text <- str_replace(dat$body_text, "\u2022.+$", "")
dat$body_text <- str_replace(dat$body_text, "\u2022.+$", "")
dat <- dat[-which(grepl("briefing", dat$headline) == TRUE),]
corp <- corpus(dat,
docid_field = "headline",
text_field = "body_text")
collocations <- get_coll(prepped_toks) # get collocations
toks <- tokens_compound(prepped_toks, pattern = collocations[collocations$z > 10,]) # replace collocations
super_stops <- c("said", # drop some additional common stopwords
"say",
"also")
super_stops <- c("said", # drop some additional common stopwords
"say",
"also")
toks <- tokens_remove(toks, super_stops,
valuetype = "glob")
toks <- tokens_remove(tokens(toks), "")
toks <- tokens(toks,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_separators = TRUE,
remove_url = TRUE) # remove other uninformative text
dfm <- dfm(toks)
dfm <- dfm_trim(dfm, min_docfreq = 20) # this can be a *very* important step
## 2. Perform STM
# Convert dfm to stm
stmdfm <- convert(dfm, to = "stm")
# Set k
K <- 8
# Run STM algorithm
modelFit <- stm(documents = stmdfm$documents,
vocab = stmdfm$vocab,
K = K,
prevalence = ~ section_name + s(month(date)),
#prevalence = ~ source + s(as.numeric(date_month)),
data = stmdfm$meta,
max.em.its = 500,
init.type = "Spectral",
seed = 2024,
verbose = TRUE)
# Load model (in case your computer is running slow...)
modelFit <- readRDS("/Users/yana/Documents/01_TRINITY/03_Quantitative Text Analysis tutorial/tutorial05/data/modelFit")
## 3. Interpret Topic model
# Inspect most probable terms in each topic
labelTopics(modelFit)
# Further interpretation: plotting frequent terms
plot.STM(modelFit,
type = "summary",
labeltype = "frex", # plot according to FREX metric
text.cex = 0.7,
main = "Topic prevalence and top terms")
plot.STM(modelFit,
type = "summary",
labeltype = "prob", # plot according to probability
text.cex = 0.7,
main = "Topic prevalence and top terms")
# Use wordcloud to visualise top terms per topic
cloud(modelFit,
topic = 1,
scale = c(2.5, 0.3),
max.words = 50)
# Reading documents with high probability topics: the findThoughts() function
findThoughts(modelFit,
texts = dfm@docvars$standfirst, # If you include the original corpus text, we could refer to this here
topics = NULL,
n = 3)
## 4. Topic validation: predictive validity using time series data
#     a) Convert metadata to correct format
#stmdfm$meta$num_month <- as.numeric(stmdfm$meta$date_month)
stmdfm$meta$num_month <- month(stmdfm$meta$date)
#     b) Aggregate topic probability by month
agg_theta <- setNames(aggregate(modelFit$theta,
by = list(month = stmdfm$meta$num_month),
FUN = mean),
c("month", paste("Topic",1:K)))
## 4. Topic validation: predictive validity using time series data
#     a) Convert metadata to correct format
stmdfm$meta$num_month <- as.numeric(stmdfm$meta$date_month)
knitr::opts_chunk$set(echo = TRUE)
library(quanteda)
library(stm) # STM
library(wordcloud)
library(ggplot2)
if(!require(devtools)) install.packages("devtools")
library("stmBrowser")
# load dfm from disk
dfm <- readRDS("/Users/yana/Documents/01_TRINITY/03_Quantitative Text Analysis/Final Project/data/dfm_anti2013_last.rds")
stmdfm <- convert(dfm, to = "stm") # convert quanteda dfm to stm format (helps with memory)
modelFit <- stm(documents = stmdfm$documents,
vocab = stmdfm$vocab,
K = 5,
prevalence = ~ 1,
#source + s(as.numeric(date_month))
data = stmdfm$meta,
max.em.its = 500,
init.type = "Spectral",
seed = 1234,
verbose = TRUE)
saveRDS(modelFit, "/Users/yana/Documents/01_TRINITY/03_Quantitative Text Analysis/Final Project/data/modelFit2013_last") # save the estimated model to disk for interpretation and usage
modelFit <- readRDS("/Users/yana/Documents/01_TRINITY/03_Quantitative Text Analysis/Final Project/data/modelFit2013_last")
labelTopics(modelFit)
plot.STM(modelFit,
type = "summary",
labeltype = "frex",
text.cex = 0.7,
main = "Topic prevalence and top terms")
plot.STM(modelFit,
type = "summary",
labeltype = "prob",
text.cex = 0.7,
main = "Topic prevalence and top terms")
cloud(modelFit,
topic = 1,
scale = c(2.5, 0.3),
max.words = 50)
findThoughts(modelFit,
texts = dfm@docvars$original_text,
topics = 3,
n = 10)
topic_correlations <- topicCorr(modelFit)
plot.topicCorr(topic_correlations,
vlabels = seq(1:ncol(modelFit$theta)),
vertex.color = "white",
main = "Topic correlations")
# note: this code is broken when used with modelFit.rds; it may work if you run your own model
topicQuality(model = modelFit,
documents = stmdfm$documents,
xlab = "Semantic Coherence",
ylab = "Exclusivity",
labels = 1:ncol(modelFit$theta),
M = 15)
# note: this code is broken when used with modelFit.rds; it may work if you run your own model
SemEx <- as.data.frame(cbind(c(1:ncol(modelFit$theta)),
exclusivity(modelFit),
semanticCoherence(model = modelFit,
documents = stmdfm$documents,
M = 15)))
colnames(SemEx) <- c("k", "ex", "coh")
SemExPlot <- ggplot(SemEx, aes(coh, ex)) +
geom_text(aes(label=k)) +
labs(x = "Semantic Coherence",
y = "Exclusivity",
title = "Topic Semantic Coherence vs. Exclusivity") +
geom_rug() +
theme_minimal() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_rect(colour = "gray", size=1))
SemExPlot
#install.packages('servr')
library("LDAvis")
toLDAvis(mod = modelFit,
docs = stmdfm$documents,
open.browser = interactive(),
reorder.topics = TRUE)
stmdfm <- convert(dfm, to = "stm") # convert quanteda dfm to stm format (helps with memory)
modelFit <- stm(documents = stmdfm$documents,
vocab = stmdfm$vocab,
K = 15,
prevalence = ~ 1,
#source + s(as.numeric(date_month))
data = stmdfm$meta,
max.em.its = 500,
init.type = "Spectral",
seed = 1234,
verbose = TRUE)
saveRDS(modelFit, "/Users/yana/Documents/01_TRINITY/03_Quantitative Text Analysis/Final Project/data/modelFit2013_last") # save the estimated model to disk for interpretation and usage
modelFit <- readRDS("/Users/yana/Documents/01_TRINITY/03_Quantitative Text Analysis/Final Project/data/modelFit2013_last")
labelTopics(modelFit)
plot.STM(modelFit,
type = "summary",
labeltype = "frex",
text.cex = 0.7,
main = "Topic prevalence and top terms")
knitr::opts_chunk$set(echo = TRUE)
library(quanteda)
library(readtext)
library(word2vec)
library(uwot)
## Load packages
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("tidyverse",
"guardianapi", # for working with the Guardian's API
"quanteda", # for QTA
"quanteda.textstats", # more Quanteda!
"quanteda.textplots", # even more Quanteda!
"readtext", # for reading in text data
"stringi", # for working with character strings
"textstem" # an alternative method for lemmatizing
), pkgTest)
### A. Using the Guardian API with R
gu_api_key() # run this interactive function
# 7c26e710-f5ed-49fe-a725-2986c6a01ffa  # We want to query the API on articles featuring "antisemitism" since Jan 1 2013
dat <- gu_content(query = "antisemitism", from_date = "2013-01-01") # making a tibble
# We'll save this data
saveRDS(dat, "/Users/yana/Documents/01_TRINITY/03_Quantitative Text Analysis/Final Project/data/df_2013_last")
# And make a duplicate to work on
df <- dat
# Take a look at the kind of object which gu_content creates.
# Try to find the column we need for our text analyses
head(df) # checking our tibble
# Print the number of rows
# print(nrow(df))
df <- df[df$type =="article" & df$section_id == "politics",] # see if you can subset the object to focus on the articles we want
which(duplicated(df$web_title) == TRUE) # sometimes there are duplicates...
df <- df[!duplicated(df$web_title),] # which we can remove
### B. Making a corpus
# We can use the corpus() function to convert our df to a quanteda corpus
corpus_guardian <- corpus(df,
docid_field = "web_title",
text_field = "body_text") # select the correct column here
# Checking our corpus
#summary(corpus_guardian, 5)
library(quanteda)
library(tibble)
library(readr)
texts_df <- df[, c("web_publication_date", "id", "web_url", "body_text", "web_title")]
# If 'source' is a constant value, say "The Guardian", you can add it like this:
#texts_df$source <- "Guardian"
texts_df <- data.frame(
date_month = df$web_publication_date,
id = df$id,
link = df$web_url,
text = df$body_text,
title = df$web_title,
source = 'Guardian'
)
write.csv(texts_df, "/Users/yana/Documents/01_TRINITY/03_Quantitative Text Analysis/Final Project/data/corpus_anti2013_last.csv", row.names = TRUE)
text_csv <- read.csv("/Users/yana/Documents/01_TRINITY/03_Quantitative Text Analysis/Final Project/data/corpus_anti2013_last.csv",
header = TRUE,
stringsAsFactors = FALSE,
quote = "\"",
fill = TRUE,
comment.char = "")
library(quanteda)
# Preprocess the text to remove HTML tags and JavaScript
text_csv$text <- gsub("<.*?>", "", text_csv$text) # Remove HTML tags
text_csv$text <- gsub("advertisementvar data.*?ba=0;ba", "", text_csv$text) # Remove JavaScript
# Create a corpus, specifying document names if available
corpus <- corpus(text_csv$text, docnames = text_csv$doc_id)
# Tokenize and preprocess
tokens <- quanteda::tokens(corpus,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_hyphens = TRUE,
remove_separators = TRUE,
remove_url = TRUE,
tolower = TRUE) %>%
tokens_remove(stopwords("english")) # Remove stop words in a single chain
# Directly create a Document-Feature Matrix (DFM) from tokens
dfm <- dfm(tokens)
docs <- as.list(tokens) # get text of articles
docs <- tolower(docs)
# Create a Document-Feature Matrix (DFM)
#dfm <- dfm(docs)
# Save the DFM object to a file
saveRDS(dfm, "/Users/yana/Documents/01_TRINITY/03_Quantitative Text Analysis/Final Project/data/dfm_anti2013_last.rds")
library(word2vec)
set.seed(12345)
model <- word2vec(x = docs,
type = "skip-gram",
dim = 300,
window = 6,
iter = 10,
threads = 15)
write.word2vec(model,
file = "/Users/yana/Documents/01_TRINITY/03_Quantitative Text Analysis/Final Project/data/anti2013_last_w2v_sg_6_300",
type = c("bin", "txt"),
encoding = "UTF-8")
knitr::opts_chunk$set(echo = TRUE)
library(quanteda)
library(stm) # STM
library(wordcloud)
library(ggplot2)
if(!require(devtools)) install.packages("devtools")
library("stmBrowser")
# load dfm from disk
dfm <- readRDS("/Users/yana/Documents/01_TRINITY/03_Quantitative Text Analysis/Final Project/data/dfm_anti2013_last.rds")
stmdfm <- convert(dfm, to = "stm") # convert quanteda dfm to stm format (helps with memory)
modelFit <- stm(documents = stmdfm$documents,
vocab = stmdfm$vocab,
K = 15,
prevalence = ~ 1,
#source + s(as.numeric(date_month))
data = stmdfm$meta,
max.em.its = 500,
init.type = "Spectral",
seed = 1234,
verbose = TRUE)
saveRDS(modelFit, "/Users/yana/Documents/01_TRINITY/03_Quantitative Text Analysis/Final Project/data/modelFit2013_last") # save the estimated model to disk for interpretation and usage
modelFit <- readRDS("/Users/yana/Documents/01_TRINITY/03_Quantitative Text Analysis/Final Project/data/modelFit2013_last")
labelTopics(modelFit)
plot.STM(modelFit,
type = "summary",
labeltype = "frex",
text.cex = 0.7,
main = "Topic prevalence and top terms")
plot.STM(modelFit,
type = "summary",
labeltype = "prob",
text.cex = 0.7,
main = "Topic prevalence and top terms")
cloud(modelFit,
topic = 1,
scale = c(2.5, 0.3),
max.words = 50)
findThoughts(modelFit,
texts = dfm@docvars$original_text,
topics = 3,
n = 10)
cloud(modelFit,
topic = 11,
scale = c(2.5, 0.3),
max.words = 50)
topic_correlations <- topicCorr(modelFit)
plot.topicCorr(topic_correlations,
vlabels = seq(1:ncol(modelFit$theta)),
vertex.color = "white",
main = "Topic correlations")
# note: this code is broken when used with modelFit.rds; it may work if you run your own model
topicQuality(model = modelFit,
documents = stmdfm$documents,
xlab = "Semantic Coherence",
ylab = "Exclusivity",
labels = 1:ncol(modelFit$theta),
M = 15)
# note: this code is broken when used with modelFit.rds; it may work if you run your own model
SemEx <- as.data.frame(cbind(c(1:ncol(modelFit$theta)),
exclusivity(modelFit),
semanticCoherence(model = modelFit,
documents = stmdfm$documents,
M = 15)))
colnames(SemEx) <- c("k", "ex", "coh")
SemExPlot <- ggplot(SemEx, aes(coh, ex)) +
geom_text(aes(label=k)) +
labs(x = "Semantic Coherence",
y = "Exclusivity",
title = "Topic Semantic Coherence vs. Exclusivity") +
geom_rug() +
theme_minimal() +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_rect(colour = "gray", size=1))
SemExPlot
labelTopics(modelFit,
topics = c(8,10))
labelTopics(modelFit,
topics = c(13,10))
stmdfm <- convert(dfm, to = "stm") # convert quanteda dfm to stm format
stmdfm$meta$num_month <- as.numeric(stmdfm$meta$date_month)
# note: this code is broken when used with modelFit.rds; it may work if you run your own model
topicQuality(model = modelFit,
documents = stmdfm$documents,
xlab = "Semantic Coherence",
ylab = "Exclusivity",
labels = 1:ncol(modelFit$theta),
M = 15)
#install.packages('servr')
library("LDAvis")
toLDAvis(mod = modelFit,
docs = stmdfm$documents,
open.browser = interactive(),
reorder.topics = TRUE)
stmBrowser(mod = modelFit,
data = stmdfm$meta,
covariates = c("source", "date_month"),
text = "original_text",
n = 1000)
